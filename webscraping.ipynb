{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naidu970/Automated-LinkedIn-Job-Listing-Scraping-with-Google-Sheets-Integration/blob/main/webscraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automated LinkedIn Job Listing Scraping with Google Sheets Integration\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k-IWS4049u1K"
      },
      "id": "k-IWS4049u1K"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b129e88b",
      "metadata": {
        "id": "b129e88b"
      },
      "outputs": [],
      "source": [
        "!pip install gspread gspread_dataframe  -q\n",
        "\n",
        "#importing required packages and libraries\n",
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26e69d61",
      "metadata": {
        "id": "26e69d61"
      },
      "outputs": [],
      "source": [
        "\n",
        "def convert_date(date):\n",
        "    \"\"\"\n",
        "    Convert a date string to a formatted date string\n",
        "\n",
        "    Parameters:\n",
        "        date (str): A date string in ISO format.\n",
        "    Returns:\n",
        "        str: A formatted date and time string in the format \"YYYY-MM-DD HH:MM:SS\".\n",
        "\n",
        "    \"\"\"\n",
        "    date_string = date[:-1]  #Removing Z\n",
        "    parsed_date = datetime.fromisoformat(date_string)\n",
        "    formatted_date = parsed_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    return formatted_date\n",
        "\n",
        "def process_text(text, replace_tupl):\n",
        "    \"\"\"\n",
        "    Replace substrings in the given text based on a tuple of tuples.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input text to be processed.\n",
        "        replace_tupl (tuple of tuple): A tuple of tuples, where each tuple contains two elements:\n",
        "            - The substring to be replaced.\n",
        "            - The replacement substring.\n",
        "\n",
        "    Returns:\n",
        "        str: The text with replacements applied based on the provided tuple of tuples.\n",
        "    \"\"\"\n",
        "    for pair in replace_tupl:\n",
        "        text = text.replace(pair[0],pair[1])\n",
        "    return text\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e45b1847",
      "metadata": {
        "id": "e45b1847",
        "outputId": "2fa20872-2d9e-4cce-c953-087babe1bd2f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████| 40/40 [01:15<00:00,  1.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Url : https://br.linkedin.com/jobs/view/assistente-administrativo-at-epharma-pbm-phygital-3746989994?refId=FOqnlTqA1AIs%2FeCYGBsUeQ%3D%3D&trackingId=DTYHRBBf%2B5BLdtDepz%2BKrw%3D%3D&position=1&pageNum=0&trk=public_jobs_jserp-result_search-card\n",
            "Listing ID : 3746989994\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# url = \"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/jobs-in-worldwide?start=\"\n",
        "url = \"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/jobs-in-worldwide?keywords=&f_TPR=&f_JT=F%2CC&f_WT=2&start=\"\n",
        "\n",
        "url_list=[] # Collect Listing Urls\n",
        "id_list = [] #Collect Listing Ids\n",
        "num = 25 #Number of Linkeldn posts per scroll\n",
        "start_num = 0 #starting number\n",
        "end_num = start_num + 1000\n",
        "for i in tqdm.tqdm(range(start_num, end_num, num)):\n",
        "    try:\n",
        "        response = requests.get(url+str(i))  #Sending request\n",
        "    except:\n",
        "        continue\n",
        "    soup = BeautifulSoup(response.content,'html.parser')\n",
        "\n",
        "    #finding the required data from the tags\n",
        "    job_title_elements = soup.find_all('a', class_='base-card__full-link')\n",
        "    job_id_elements= soup.find_all('div', attrs={\"data-entity-urn\": True})\n",
        "    job_location = soup.find_all('span',class_='job-search-card__location')\n",
        "\n",
        "\n",
        "    for job_loc,job_id,job_title in zip(job_location,job_id_elements,job_title_elements):\n",
        "        if (\"United States\" in job_loc.text) or  (\"Canada\" in job_loc.text): #No job selection from United States and Canada\n",
        "            continue\n",
        "        #appending extracted data to the lists\n",
        "        id_list.append(job_id['data-entity-urn'].split(\":\")[-1])\n",
        "        url_list.append(job_title.get('href'))\n",
        "\n",
        "\n",
        "print(\"Url :\",url_list[0])\n",
        "print(\"Listing ID :\",id_list[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22fe2478",
      "metadata": {
        "id": "22fe2478",
        "outputId": "4b8d3557-1041-4c74-c41f-92ec09fc3b3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|████████                                 | 173/874 [02:56<10:06,  1.16it/s]"
          ]
        }
      ],
      "source": [
        "#DataFrame to store extracted data\n",
        "df = pd.DataFrame({'company_id':[],\n",
        "                'listing_id':[],\n",
        "             'listing_url':[],\n",
        "             'job_posting_title':[],\n",
        "             'seniority_level':[],\n",
        "             'employment_type':[],\n",
        "             'job_function':[],\n",
        "             'industry':[],\n",
        "             'date_posted':[],\n",
        "             'description':[],\n",
        "             'hiring_organization_name':[],\n",
        "             'hiring_organization_url':[],\n",
        "             'job_location_address_type':[],\n",
        "             'job_location_address_country':[],\n",
        "             'job_location_address_locality':[],\n",
        "             'job_name':[],\n",
        "             'job_location_address_locality':[],\n",
        "              'job_post_valid_date':[]\n",
        "             })\n",
        "\n",
        "#tuple of tuple to process text\n",
        "replace_tupl = (('li',''),('li',''),(';strong',''),('/strong',''),('ul',''),('gt',' '),('br',''),\\\n",
        "                ('lt',''),('/',''),('&',''),(';',''),('  ','\\n'),('\\n\\n','\\n'))\n",
        "\n",
        "\n",
        "\n",
        "for num in tqdm.tqdm(range(len(url_list))):\n",
        "\n",
        "    try:\n",
        "        job_post_link = url_list[num]\n",
        "        job_id = id_list[num]\n",
        "\n",
        "        response = requests.get(job_post_link)  #Sending request\n",
        "        soup = BeautifulSoup(response.content,'html.parser')\n",
        "\n",
        "        #finding the required data from the tags\n",
        "        job_attr_val= soup.find_all('span', class_='description__job-criteria-text description__job-criteria-text--criteria')\n",
        "        job_attr_name = soup.find_all('h3', class_='description__job-criteria-subheader')\n",
        "        job_attr = {attr_name : 'not applicable' for attr_name in ['Seniority level','Employment type','Job function','Industries']}\n",
        "\n",
        "        for name, val in (zip(job_attr_name,job_attr_val)):\n",
        "            #extracting and storing data\n",
        "            job_attr[name.get_text(strip=True)] = val.get_text(strip=True)\n",
        "\n",
        "\n",
        "        try:\n",
        "            #extract job post essential data as string\n",
        "            str_json = soup.find('script',{'type':'application/ld+json'}).text\n",
        "            #loading into json file\n",
        "            dict_job = json.loads(str_json)\n",
        "\n",
        "            #Job Location except from US and CA\n",
        "            if (dict_job['jobLocation']['address']['addressCountry'] in [\"US\",\"CA\"]):\n",
        "                continue\n",
        "            #appending to the dataframe\n",
        "            df.loc[len(df)] = (soup.find(\"meta\",{'name':'companyId'})['content'],\\\n",
        "                               job_id,\\\n",
        "                               f\"https://www.linkedin.com/jobs/view/{job_id}\",\\\n",
        "                               soup.find('title').text,\\\n",
        "                               job_attr['Seniority level'],\\\n",
        "                               job_attr['Employment type'],\\\n",
        "                               job_attr['Job function'],\\\n",
        "                               job_attr['Industries'],\\\n",
        "                               convert_date(dict_job['datePosted']),\\\n",
        "                               process_text(dict_job['description'],replace_tupl),\\\n",
        "                               dict_job['hiringOrganization']['name'],\\\n",
        "                               dict_job['hiringOrganization']['sameAs'],\\\n",
        "                               dict_job['jobLocation']['address']['@type'],\\\n",
        "                               dict_job['jobLocation']['address']['addressCountry'],\\\n",
        "                               dict_job['jobLocation']['address']['addressLocality'],\\\n",
        "                               dict_job['title'],\\\n",
        "                               convert_date(dict_job['validThrough']))\n",
        "        except AttributeError:\n",
        "            pass\n",
        "\n",
        "    except :\n",
        "        print(\"bad request\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f27ac296",
      "metadata": {
        "id": "f27ac296"
      },
      "outputs": [],
      "source": [
        "#shape of Dataframe\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df7a3a63",
      "metadata": {
        "id": "df7a3a63"
      },
      "outputs": [],
      "source": [
        "#using api to authorize changes in file\n",
        "sa = gspread.service_account(filename=\"python-web-scraping-403619-fcfb4d21578b.json\")\n",
        "#selecting google sheets file\n",
        "sheet =sa.open(\"linkeldn-job-listing\")\n",
        "#selecting required sheet\n",
        "worksheet = sheet.worksheet(\"Sheet1\")\n",
        "\n",
        "#columns to find duplicates on\n",
        "columns = ['company_id','listing_id', 'hiring_organization_name', 'job_name', 'listing_url',\\\n",
        "       'job_location_address_country', 'job_location_address_locality']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f114fda0",
      "metadata": {
        "id": "f114fda0"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    #converting sheet into dataframe\n",
        "    df_existing = pd.DataFrame(worksheet.get_all_records())\n",
        "    #concating existing dataframe and newly acquired data\n",
        "    df_tmp =pd.concat([df_existing,df],axis=0).reset_index(drop=True)\n",
        "    #removing duplicates\n",
        "    sub_df = df_tmp[~df_tmp.duplicated(columns, keep='first')].reset_index(drop=True)\n",
        "    #uploading new data + old data into the sheet\n",
        "    set_with_dataframe(worksheet=worksheet, dataframe=sub_df, include_index=False,\\\n",
        "                       include_column_header=True, resize=True)\n",
        "except IndexError:\n",
        "    #as sheet is empty initally\n",
        "    print('Index Error')\n",
        "    #uploading new data into sheet\n",
        "    set_with_dataframe(worksheet=worksheet, dataframe=df, include_index=False,\\\n",
        "                       include_column_header=True, resize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3425a40",
      "metadata": {
        "id": "a3425a40"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}